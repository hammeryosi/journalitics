{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from math import ceil\n",
    "\n",
    "class scraper:\n",
    "    def __init__(self):\n",
    "        self.baseURL = 'http://pqasb.pqarchiver.com'\n",
    "        self.waitTime = 30\n",
    "\n",
    "    def dateUrl(self, date):\n",
    "        y, m, d = date.split('-')\n",
    "        return (self.baseURL +\n",
    "                '/latimes/results.html?st=advanced&QryTxt=*&datetype=6' +\n",
    "                '&frommonth=' + m + '&fromday=' + d + '&fromyear=' + y +\n",
    "                '&tomonth=' + m + '&today=' + d + '&toyear=' + y)\n",
    "\n",
    "    def fetchPage(self, url):\n",
    "        success = False\n",
    "        while not success:\n",
    "            page = requests.get(url).text\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "            if not (soup.title.text ==\n",
    "                    '503 Service Temporarily Unavailable'):\n",
    "                success = True\n",
    "            else:\n",
    "                print('Failed fetching page:\\n'\n",
    "                      + url + '\\nRetrying in ' +\n",
    "                      str(self.waitTime) + ' seconds..')\n",
    "                time.sleep(self.waitTime)\n",
    "        return soup\n",
    "\n",
    "    def titlesInPage(self, soup, start):\n",
    "        resultTable = soup.find(text=str(start) + '.').parent.parent.parent.parent\n",
    "        trList = resultTable.findAll('tr')\n",
    "        els = [trList[2 * i] for i in range(int(len(trList) / 2))]\n",
    "        return els\n",
    "\n",
    "    def parseElement(self, el):\n",
    "        journal = 'NYP'\n",
    "        link = self.baseURL + el.find_all('td')[1].a['href']\n",
    "        print(link)\n",
    "        soup = self.fetchPage(link)\n",
    "        summary = soup.find(text=' (Document Summary)')\n",
    "        if summary is not None:\n",
    "            summary = re.sub('[\\n|\\t]', '', summary.next.next.text)\n",
    "        else:\n",
    "            summary = ''\n",
    "        title = soup.find(class_='docTitle').text\n",
    "        authorAnchor = soup.find(lambda x: x.text == 'Author:')\n",
    "        if authorAnchor is not None:\n",
    "            author = authorAnchor.next.next.next.text\n",
    "            author = ','.join([' '.join(reversed(x.split(', '))) for x in author.split('||||||')])\n",
    "        else:\n",
    "            author = ''\n",
    "        section = soup.find(lambda x: x.text == 'Section:').next.next.next.text\n",
    "        return {'link': link,\n",
    "                'summary': summary,\n",
    "                'title': title,\n",
    "                'author': author,\n",
    "                'section': section,\n",
    "                'journal': journal}\n",
    "\n",
    "\n",
    "\n",
    "    def getTitles(self, date):\n",
    "        url = self.dateUrl(date)\n",
    "        soup = self.fetchPage(url)\n",
    "        if soup.find(text='No Articles Found') is not None:\n",
    "            numOfResults = 0\n",
    "        else:\n",
    "            numOfResults = (\n",
    "                int(soup.find(text=' to ').parent('b')[2].text))\n",
    "        print(date + ': ' + str(numOfResults) + ' results')\n",
    "        if numOfResults > 0:\n",
    "            # titles from first page\n",
    "            els = self.titlesInPage(soup, 1)\n",
    "            headlines = [self.parseElement(el) for\n",
    "                         el in els]\n",
    "            print('page 1: ' + str(len(els)) + ' articles')\n",
    "        else:\n",
    "            headlines = []\n",
    "        if numOfResults > 10:\n",
    "            pages = int(ceil(numOfResults / 10.))\n",
    "            for p in range(1,pages):\n",
    "                start = p * 10\n",
    "                pageUrl = url + '&start=' + str(start)\n",
    "                soup = self.fetchPage(pageUrl)\n",
    "                if soup.find(text='No Articles Found') is None:\n",
    "                    els = self.titlesInPage(soup, start + 1)\n",
    "                    headlines += [self.parseElement(el) for\n",
    "                         el in els]\n",
    "                    print('page ' + str(p + 1) +\n",
    "                        ': ' + str(len(els)) + ' articles')\n",
    "                else:\n",
    "                    break\n",
    "        for h in headlines:\n",
    "            h['date'] = date\n",
    "        return headlines\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-11: 105 results\n",
      "http://pqasb.pqarchiver.com/latimes/doc/1747522743.html?FMT=ABS&FMTS=ABS:FT&date=Dec+11%2C+2015&author=Kirkham%2C+Chris%7C%7C%7C%7C%7C%7CPenn%2C+Ivan&pub=Los+Angeles+Times&edition=&startpage=C.1&desc=Electric+car+maker+picks+Nevada+for+new+factory%3B+Faraday+Future+will+build+a+%241-billion+plant+in+North+Las+Vegas.+California+loses+out.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-cdab1559ca46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtitles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetTitles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2015-12-11\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-bcb0cbe9e5cf>\u001b[0m in \u001b[0;36mgetTitles\u001b[1;34m(self, date)\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitlesInPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             headlines = [self.parseElement(el) for\n\u001b[1;32m---> 80\u001b[1;33m                          el in els]\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'page 1: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' articles'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-bcb0cbe9e5cf>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitlesInPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             headlines = [self.parseElement(el) for\n\u001b[1;32m---> 80\u001b[1;33m                          el in els]\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'page 1: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' articles'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-bcb0cbe9e5cf>\u001b[0m in \u001b[0;36mparseElement\u001b[1;34m(self, el)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' (Document Summary)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msummary\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[\\n|\\t]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "titles = sc.getTitles(\"2015-12-11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
