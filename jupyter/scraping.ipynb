{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from math import ceil\n",
    "\n",
    "class scraper:\n",
    "    def __init__(self):\n",
    "        self.baseURL = 'http://pqasb.pqarchiver.com'\n",
    "        self.waitTime = 60\n",
    "        self.jName = 'WP'\n",
    "\n",
    "    def dateUrl(self, date):\n",
    "        y, m, d = date.split('-')\n",
    "        return (self.baseURL +\n",
    "                '/washingtonpost/results.html?st=advanced&QryTxt=*&datetype=6' +\n",
    "                '&frommonth=' + m + '&fromday=' + d + '&fromyear=' + y +\n",
    "                '&tomonth=' + m + '&today=' + d + '&toyear=' + y)\n",
    "\n",
    "    def fetchPage(self, url):\n",
    "        success = False\n",
    "        while not success:\n",
    "            page = requests.get(url).text\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "            if not (soup.title.text ==\n",
    "                    '503 Service Temporarily Unavailable'):\n",
    "                success = True\n",
    "            else:\n",
    "                print('server unavailable, retrying in ' +\n",
    "                      str(self.waitTime) + ' seconds..')\n",
    "                time.sleep(self.waitTime)\n",
    "        return soup\n",
    "\n",
    "    def titlesInPage(self, soup, start):\n",
    "        resultTable = soup.find(text=str(start) + '.').parent.parent.parent.parent\n",
    "        trList = resultTable.findAll('tr')\n",
    "        els = [trList[2 * i] for i in range(int(len(trList) / 2))]\n",
    "        return els\n",
    "\n",
    "    def parseElement(self, el):\n",
    "        journal = self.jName\n",
    "        link = self.baseURL + el.find_all('td')[1].a['href']\n",
    "        soup = self.fetchPage(link)\n",
    "        summary = soup.find(text=' (Document Summary)')\n",
    "        if summary is not None:\n",
    "            summary = re.sub('[\\n|\\t]', '', summary.next.next.text)\n",
    "        else:\n",
    "            summary = ''\n",
    "        title = soup.find(class_='docTitle').text\n",
    "        authorAnchor = soup.find(lambda x: x.text == 'Author:')\n",
    "        if authorAnchor is not None:\n",
    "            author = authorAnchor.next.next.next.text\n",
    "            author = ','.join([' '.join(reversed(x.split(', '))) for x in author.split('||||||')])\n",
    "        else:\n",
    "            author = ''\n",
    "        section = soup.find(lambda x: x.text == 'Section:').next.next.next.text\n",
    "        return {'link': link,\n",
    "                'summary': summary,\n",
    "                'title': title,\n",
    "                'author': author,\n",
    "                'section': section,\n",
    "                'journal': journal}\n",
    "\n",
    "\n",
    "\n",
    "    def getTitles(self, date):\n",
    "        url = self.dateUrl(date)\n",
    "        soup = self.fetchPage(url)\n",
    "        if soup.find(text='No Articles Found') is not None:\n",
    "            numOfResults = 0\n",
    "        else:\n",
    "            numOfResults = (\n",
    "                int(soup.find(text=' to ').parent('b')[2].text))\n",
    "        print(date + ': ' + str(numOfResults) + ' results')\n",
    "        if numOfResults > 0:\n",
    "            # titles from first page\n",
    "            els = self.titlesInPage(soup, 1)\n",
    "            headlines = [self.parseElement(el) for\n",
    "                         el in els]\n",
    "            print('page 1: ' + str(len(els)) + ' articles')\n",
    "        else:\n",
    "            headlines = []\n",
    "        if numOfResults > 10:\n",
    "            pages = int(ceil(numOfResults / 10.))\n",
    "            for p in range(1,pages):\n",
    "                start = p * 10\n",
    "                pageUrl = url + '&start=' + str(start)\n",
    "                soup = self.fetchPage(pageUrl)\n",
    "                if soup.find(text='No Articles Found') is None:\n",
    "                    els = self.titlesInPage(soup, start + 1)\n",
    "                    headlines += [self.parseElement(el) for\n",
    "                         el in els]\n",
    "                    print('page ' + str(p + 1) +\n",
    "                        ': ' + str(len(els)) + ' articles')\n",
    "                else:\n",
    "                    break\n",
    "        for h in headlines:\n",
    "            h['date'] = date\n",
    "        return headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'parent'",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-dbefc5c94761>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtitles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetTitles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2012-01-01\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-7e4181e9dc5c>\u001b[0m in \u001b[0;36mgetTitles\u001b[1;34m(self, date)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             numOfResults = (\n\u001b[1;32m---> 73\u001b[1;33m                 int(soup.find(text=' to ').parent('b')[2].text))\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumOfResults\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' results'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumOfResults\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'parent'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "titles = sc.getTitles(\"2012-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = sc.dateUrl(\"2012-01-01\")\n",
    "page = requests.get(url).text\n",
    "soup = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'parent'",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f5f2affe867b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m numOfResults = (\n\u001b[1;32m----> 2\u001b[1;33m                 int(soup.find(text=' to ').parent('b')[2].text))\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumOfResults\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' results'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'parent'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "numOfResults = (\n",
    "                int(soup.find(text=' to ').parent('b')[2].text))\n",
    "print(date + ': ' + str(numOfResults) + ' results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(text=' to ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}